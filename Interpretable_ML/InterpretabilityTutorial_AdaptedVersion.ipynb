{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Machine Leaning with Python\n",
    "Adapted from [this tutorial](http://savvastjortjoglou.com/intrepretable-machine-learning-nfl-combine.html#PDP-and-ICE-plots) by Savvas Tjortjoglou <br>\n",
    "\n",
    "In this tutorial, you will:\n",
    "1. Train a Random Forest Regression model on data about football players\n",
    "2. Learn about Mean Decrease Impurity, Mean Decrease Accuracy  importance, and how to apply them to the model\n",
    "3. Learn how to evaluate and interpret feature contributions globally, and for single instances (decision paths), and various methods to visualize contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train a Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps: <br>\n",
    "A. Create a conda environment with the packages we need (for whole tutorial) <br>\n",
    "B. Import packages (for whole tutorial)<br>\n",
    "C. Load in and subset the data we want to use in our model <br>\n",
    "D. Split training and testing data <br>\n",
    "E. Build the model <br>\n",
    "F. Tune model parameters and train <br>\n",
    "G. Apply model to test data and evaluate <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create conda Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Install in conda environment\n",
    "# conda install -c conda-forge -c ets skll\n",
    "# conda install -c conda-forge scikit-optimize\n",
    "# conda install -c conda-forge tqdm \n",
    "# conda install -c conda-forge eli5\n",
    "# conda install -c conda-forge graphviz \n",
    "# conda install -c conda-forge pydotplus\n",
    "# pip install treeinterpreter\n",
    "# pip install pycebox\n",
    "# conda install -c conda-forge pdpbox \n",
    "# conda install -c conda-forge xgboost <-- failed for Shinhan, worked for Serena \n",
    "# pip install xgboost <-- failed for Serena, worked for Shinhan\n",
    "# conda install -c conda-forge lime \n",
    "# conda install -c conda-forge shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer # this package is called scikit-learn\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV # this package is called scikit-optimize\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the aesthetic for the plots in this notebook\n",
    "sns.set(style=\"white\", palette=\"colorblind\", font_scale=1.2, \n",
    "        rc={\"figure.figsize\":(8,6)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Load in and subset data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we're trying to predict the 'Approximate Value' (AV) of football players: an explanation can be found [here](https://www.pro-football-reference.com/about/glossary.htm). We are doing this using a dataset where instances are individual players, and there are various performance metrics for each player, which we will later use as features in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>Forty</th>\n",
       "      <th>Vertical</th>\n",
       "      <th>BenchReps</th>\n",
       "      <th>BroadJump</th>\n",
       "      <th>Cone</th>\n",
       "      <th>Shuttle</th>\n",
       "      <th>Year</th>\n",
       "      <th>Pfr_ID</th>\n",
       "      <th>AV</th>\n",
       "      <th>Team</th>\n",
       "      <th>Round</th>\n",
       "      <th>Pick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Abraham</td>\n",
       "      <td>OLB</td>\n",
       "      <td>76</td>\n",
       "      <td>252</td>\n",
       "      <td>4.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>AbraJo00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>New York Jets</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shaun Alexander</td>\n",
       "      <td>RB</td>\n",
       "      <td>72</td>\n",
       "      <td>218</td>\n",
       "      <td>4.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>AlexSh00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Seattle Seahawks</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Darnell Alford</td>\n",
       "      <td>OT</td>\n",
       "      <td>76</td>\n",
       "      <td>334</td>\n",
       "      <td>5.56</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>8.48</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2000</td>\n",
       "      <td>AlfoDa20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kansas City Chiefs</td>\n",
       "      <td>6.0</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kyle Allamon</td>\n",
       "      <td>TE</td>\n",
       "      <td>74</td>\n",
       "      <td>253</td>\n",
       "      <td>4.97</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.0</td>\n",
       "      <td>7.29</td>\n",
       "      <td>4.49</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rashard Anderson</td>\n",
       "      <td>CB</td>\n",
       "      <td>74</td>\n",
       "      <td>206</td>\n",
       "      <td>4.55</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.0</td>\n",
       "      <td>7.18</td>\n",
       "      <td>4.15</td>\n",
       "      <td>2000</td>\n",
       "      <td>AndeRa21</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Player  Pos  Ht   Wt  Forty  Vertical  BenchReps  BroadJump  \\\n",
       "0      John Abraham  OLB  76  252   4.55       NaN        NaN        NaN   \n",
       "1   Shaun Alexander   RB  72  218   4.58       NaN        NaN        NaN   \n",
       "2    Darnell Alford   OT  76  334   5.56      25.0       23.0       94.0   \n",
       "3      Kyle Allamon   TE  74  253   4.97      29.0        NaN      104.0   \n",
       "4  Rashard Anderson   CB  74  206   4.55      34.0        NaN      123.0   \n",
       "\n",
       "   Cone  Shuttle  Year    Pfr_ID    AV                Team  Round   Pick  \n",
       "0   NaN      NaN  2000  AbraJo00  26.0       New York Jets    1.0   13.0  \n",
       "1   NaN      NaN  2000  AlexSh00  26.0    Seattle Seahawks    1.0   19.0  \n",
       "2  8.48     4.98  2000  AlfoDa20   0.0  Kansas City Chiefs    6.0  188.0  \n",
       "3  7.29     4.49  2000       NaN   0.0                 NaN    NaN    NaN  \n",
       "4  7.18     4.15  2000  AndeRa21   6.0   Carolina Panthers    1.0   23.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data: modify path if data isn't in currend working directory\n",
    "data_df = pd.read_csv('combine_data_since_2000_PROCESSED_2018-04-26.csv')\n",
    "\n",
    "# make a subset of players that have been in the league for 3+ years, we will use this for our model\n",
    "data_df2 = data_df.loc[data_df.Year <= 2015].copy()\n",
    "data_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>Forty</th>\n",
       "      <th>Vertical</th>\n",
       "      <th>BenchReps</th>\n",
       "      <th>BroadJump</th>\n",
       "      <th>Cone</th>\n",
       "      <th>Shuttle</th>\n",
       "      <th>Year</th>\n",
       "      <th>Pfr_ID</th>\n",
       "      <th>AV</th>\n",
       "      <th>Team</th>\n",
       "      <th>Round</th>\n",
       "      <th>Pick</th>\n",
       "      <th>AV_pctile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Abraham</td>\n",
       "      <td>OLB</td>\n",
       "      <td>76</td>\n",
       "      <td>252</td>\n",
       "      <td>4.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>AbraJo00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>New York Jets</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.951482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shaun Alexander</td>\n",
       "      <td>RB</td>\n",
       "      <td>72</td>\n",
       "      <td>218</td>\n",
       "      <td>4.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>AlexSh00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Seattle Seahawks</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.936123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Darnell Alford</td>\n",
       "      <td>OT</td>\n",
       "      <td>76</td>\n",
       "      <td>334</td>\n",
       "      <td>5.56</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>8.48</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2000</td>\n",
       "      <td>AlfoDa20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kansas City Chiefs</td>\n",
       "      <td>6.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kyle Allamon</td>\n",
       "      <td>TE</td>\n",
       "      <td>74</td>\n",
       "      <td>253</td>\n",
       "      <td>4.97</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.0</td>\n",
       "      <td>7.29</td>\n",
       "      <td>4.49</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rashard Anderson</td>\n",
       "      <td>CB</td>\n",
       "      <td>74</td>\n",
       "      <td>206</td>\n",
       "      <td>4.55</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.0</td>\n",
       "      <td>7.18</td>\n",
       "      <td>4.15</td>\n",
       "      <td>2000</td>\n",
       "      <td>AndeRa21</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.630058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Player  Pos  Ht   Wt  Forty  Vertical  BenchReps  BroadJump  \\\n",
       "0      John Abraham  OLB  76  252   4.55       NaN        NaN        NaN   \n",
       "1   Shaun Alexander   RB  72  218   4.58       NaN        NaN        NaN   \n",
       "2    Darnell Alford   OT  76  334   5.56      25.0       23.0       94.0   \n",
       "3      Kyle Allamon   TE  74  253   4.97      29.0        NaN      104.0   \n",
       "4  Rashard Anderson   CB  74  206   4.55      34.0        NaN      123.0   \n",
       "\n",
       "   Cone  Shuttle  Year    Pfr_ID    AV                Team  Round   Pick  \\\n",
       "0   NaN      NaN  2000  AbraJo00  26.0       New York Jets    1.0   13.0   \n",
       "1   NaN      NaN  2000  AlexSh00  26.0    Seattle Seahawks    1.0   19.0   \n",
       "2  8.48     4.98  2000  AlfoDa20   0.0  Kansas City Chiefs    6.0  188.0   \n",
       "3  7.29     4.49  2000       NaN   0.0                 NaN    NaN    NaN   \n",
       "4  7.18     4.15  2000  AndeRa21   6.0   Carolina Panthers    1.0   23.0   \n",
       "\n",
       "   AV_pctile  \n",
       "0   0.951482  \n",
       "1   0.936123  \n",
       "2   0.002611  \n",
       "3   0.003509  \n",
       "4   0.630058  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the player AV percentiles by position\n",
    "data_df2['AV_pctile'] = (data_df2.groupby('Pos').AV.rank(pct=True,\n",
    "                                                         method='min', \n",
    "                                                         ascending=True))\n",
    "data_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>Forty</th>\n",
       "      <th>Vertical</th>\n",
       "      <th>BenchReps</th>\n",
       "      <th>BroadJump</th>\n",
       "      <th>Cone</th>\n",
       "      <th>Shuttle</th>\n",
       "      <th>Year</th>\n",
       "      <th>Pfr_ID</th>\n",
       "      <th>AV</th>\n",
       "      <th>Team</th>\n",
       "      <th>Round</th>\n",
       "      <th>Pick</th>\n",
       "      <th>AV_pctile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael Boireau</td>\n",
       "      <td>DE</td>\n",
       "      <td>76</td>\n",
       "      <td>274</td>\n",
       "      <td>5.09</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>7.68</td>\n",
       "      <td>4.49</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Courtney Brown</td>\n",
       "      <td>DE</td>\n",
       "      <td>77</td>\n",
       "      <td>269</td>\n",
       "      <td>4.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>BrowCo22</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Cleveland Browns</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lamont Bryant</td>\n",
       "      <td>DE</td>\n",
       "      <td>75</td>\n",
       "      <td>260</td>\n",
       "      <td>4.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>BryaLa00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leonardo Carson</td>\n",
       "      <td>DE</td>\n",
       "      <td>73</td>\n",
       "      <td>283</td>\n",
       "      <td>5.06</td>\n",
       "      <td>28.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>7.82</td>\n",
       "      <td>4.75</td>\n",
       "      <td>2000</td>\n",
       "      <td>CarsLe20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>San Diego Chargers</td>\n",
       "      <td>4.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.730583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rameel Connor</td>\n",
       "      <td>DE</td>\n",
       "      <td>75</td>\n",
       "      <td>276</td>\n",
       "      <td>5.00</td>\n",
       "      <td>35.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>7.65</td>\n",
       "      <td>4.45</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Player Pos  Ht   Wt  Forty  Vertical  BenchReps  BroadJump  Cone  \\\n",
       "0  Michael Boireau  DE  76  274   5.09      29.0       26.0      105.0  7.68   \n",
       "1   Courtney Brown  DE  77  269   4.78       NaN        NaN        NaN   NaN   \n",
       "2    Lamont Bryant  DE  75  260   4.91       NaN       17.0        NaN   NaN   \n",
       "3  Leonardo Carson  DE  73  283   5.06      28.0       22.0      106.0  7.82   \n",
       "4    Rameel Connor  DE  75  276   5.00      35.5       25.0      121.0  7.65   \n",
       "\n",
       "   Shuttle  Year    Pfr_ID    AV                Team  Round   Pick  AV_pctile  \n",
       "0     4.49  2000       NaN   0.0                 NaN    NaN    NaN   0.002427  \n",
       "1      NaN  2000  BrowCo22  16.0    Cleveland Browns    1.0    1.0   0.871359  \n",
       "2      NaN  2000  BryaLa00   0.0                 NaN    NaN    NaN   0.002427  \n",
       "3     4.75  2000  CarsLe20  10.0  San Diego Chargers    4.0  113.0   0.730583  \n",
       "4     4.45  2000       NaN   0.0                 NaN    NaN    NaN   0.002427  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data for the football position we want, in this case it's DE \n",
    "pos_df = data_df2.loc[data_df2.Pos=='DE'].copy().reset_index(drop=True)\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Split testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df = pos_df.loc[pos_df.Year <= 2011]\n",
    "test_df  = pos_df.loc[pos_df.Year.isin([2012, 2013, 2014, 2015])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of feature names\n",
    "features = ['Forty', 'Wt', 'Ht', 'Vertical', 'BenchReps', 'BroadJump', 'Cone', 'Shuttle']\n",
    "# define what we want to predict (label)\n",
    "target   = 'AV_pctile'\n",
    "\n",
    "# separate feature (X) and label (y) values from training set\n",
    "X = train_df[features].values\n",
    "y = train_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature values: [[  5.09 274.    76.    29.    26.   105.     7.68   4.49]\n",
      " [  4.78 269.    77.      nan    nan    nan    nan    nan]\n",
      " [  4.91 260.    75.      nan  17.      nan    nan    nan]\n",
      " [  5.06 283.    73.    28.    22.   106.     7.82   4.75]\n",
      " [  5.   276.    75.    35.5   25.   121.     7.65   4.45]]\n",
      "Label values: [0.00242718 0.87135922 0.00242718 0.73058252 0.00242718]\n"
     ]
    }
   ],
   "source": [
    "# What do X and y look like?\n",
    "print(\"Feature values: {}\".format(X[:5,:])) # this is a numpy array\n",
    "print(\"Label values: {}\".format(y[:5])) # this is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "In this tutorial we're going to use scikit-learn's `Pipeline` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` is a method that combines a customizable number of steps in a machine learning pipeline into one. [This paper](https://arxiv.org/pdf/1309.0238.pdf) on scikit-learn's API has a good explanation of what `Pipeline` is, on page 8-9. A brief explanation with definitions (quotes are from the API paper):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Definitions (from the scikit-learn glossary):* <br>\n",
    "\n",
    "**estimator**:<br>\n",
    "An object which manages the estimation and decoding of a model. The model is estimated as a deterministic function of:\n",
    "- parameters provided in object construction or with set_params;\n",
    "- the global numpy.random random state if the estimator’s random_state parameter is set to None; and\n",
    "- any data or sample properties passed to the most recent call to fit, fit_transform or fit_predict, or data similarly passed in a sequence of calls to partial_fit.\n",
    "\n",
    "The estimated model is stored in public and private attributes on the estimator instance, facilitating decoding through prediction and transformation methods. <br>\n",
    "\n",
    "Estimators must provide a fit method, and should provide set_params and get_params, although these are usually provided by inheritance from base.BaseEstimator. <br>\n",
    "\n",
    "The core functionality of some estimators may also be available as a function. <br>\n",
    "\n",
    "**predictor**:<br>\n",
    "An estimator supporting predict and/or fit_predict. This encompasses classifier, regressor, outlier detector and clusterer.\n",
    "\n",
    "In statistics, “predictors” refers to features.\n",
    "\n",
    "**transformer**: <br>\n",
    "An estimator supporting transform and/or fit_transform. A purely transductive transformer, such as manifold.TSNE, may not implement transform.\n",
    "\n",
    "(Note: This is *not* the NLP deep learning method Transformer that works on sequence transduction. See post [here](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65))<br>\n",
    "\n",
    "An **imputer** is not in the glossary, but generally an imputer is anything that infers missing values from the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation:*<br>\n",
    "`Pipeline` chains multiple estimators (transformers and predictors) together. \"A sequence of N such\n",
    "steps can be combined into a Pipeline if the first N − 1 steps are transformers;\n",
    "the last can be either a predictor, a transformer or both.\"<br>\n",
    "\n",
    "In other words, `Pipeline` is used to combine multiple steps in a machine learning pipeline into one, and this does not have to include the model itself; it can be used to pre-process data. The `Pipeline` acts as whatever its last step is. \"The pipeline exposes all the\n",
    "methods the last estimator in the pipe exposes. That is, if the last estimator is\n",
    "a predictor, the pipeline can itself be used as a predictor. If the last estimator is\n",
    "a transformer, then the pipeline is itself a transformer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're using the `Pipeline` object to encompass our imputer (used to impute the nan's we saw in the training data) and our predictor (the Random Forest Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make our pipeline for this model\n",
    "\n",
    "# Since we're using RF, need to define our random state\n",
    "# the RANDOM_STATE parameter for RF models is (from the scikit-optimize docs):\n",
    "# Pseudo random number generator state used for random uniform sampling from lists\n",
    "# of possible values instead of scipy.stats distributions\n",
    "RANDOM_STATE = 420 \n",
    "pipe = Pipeline([(\"imputer\", SimpleImputer()), \n",
    "                 (\"estimator\", RandomForestRegressor(random_state=RANDOM_STATE))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Tune model parameters and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **parameters** of a machine learning model are internal to the model, and optimal values are learned during the training process. However, machine learning models also have **hyperparameters**, or, parameters that are important to the model's performance, but can't be learned from the data (for example, how many trees should be in our random forest?). In order to choose the optimal hyperparameters, we can perform one of several methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**: Test every possible combination of hyperparameters. This is very intensive for most models.<br>\n",
    "**Random Search**: Randomly choose hyperparameters from a distribution. Has better and faster performance than Grid Search. Explanation of why random search is better can be found [here](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)<br>\n",
    "**Bayes Search**: Uses Bayesian optimization to search for hyperparameters. Better and faster than both the above methods. A detailed explanation can be found [here](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll use the `BayesSearchCV` method from scikit-optimize (Docs [here](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html#skopt.BayesSearchCV\n",
    "))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define the parameter space over which to search \n",
    "rf_param_space = {\n",
    "    'imputer__strategy': Categorical(['mean', 'median', 'most_frequent']),\n",
    "    'estimator__max_features': Integer(1, 8),\n",
    "    'estimator__n_estimators': Integer(50, 500), \n",
    "    'estimator__min_samples_split': Integer(2, 200),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we define a scoring function that will be used to evaluate the performance of the model with \n",
    "# various hyperparameters. r2 is Pearson's correlation coefficient.\n",
    "from sklearn.metrics import r2_score  \n",
    "r2 = make_scorer(r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note about make_scorer:\n",
    "From the docs:<br>\n",
    "You can generate even more flexible model scorers by constructing your own scoring object from scratch, without using the make_scorer factory. For a callable to be a scorer, it needs to meet the protocol specified by the following two rules:\n",
    "1. It can be called with parameters (estimator, X, y), where estimator is the model that should be evaluated, X is validation data, and y is the ground truth target for X (in the supervised case) or None (in the unsupervised case).\n",
    "2. It returns a floating point number that quantifies the estimator prediction quality on X, with reference to y. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.\n",
    "\n",
    "This is why, when we call r2 later on, we call it as: \n",
    "\n",
    "    r2(search, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our Bayes search object using the Sklearn-Optimize package\n",
    "\n",
    "N_JOBS=8\n",
    "\n",
    "search = BayesSearchCV(pipe,                      # Estimator\n",
    "                       rf_param_space,            # Search_space\n",
    "                       cv=10,                     # Cross-validation generator or an iterable\n",
    "                       n_jobs=N_JOBS,             # Number of job run in parallel \n",
    "                       verbose=0, \n",
    "                       error_score=-9999,         # Value to assign to the score if an error occurs in estimator fitting.\n",
    "                       scoring=r2,                # Loss function\n",
    "                       random_state=RANDOM_STATE, # For random uniform sampling\n",
    "                       return_train_score=True,   # The cv_results_ attribute will include training scores.\n",
    "                       n_iter=10)                 # Number of parameter settings that are sampled.\n",
    "\n",
    "# n_iter is set very low in order to run the whole thing faster, would nnormally use much higher number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the output of `BayesSearchCV( )`? i.e., what is in our object `search`? <br>\n",
    "\n",
    "*Attributes:* <br>\n",
    "**cv_results_**: dict of numpy (masked) ndarrays <br>\n",
    "A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame. <br>\n",
    "\n",
    "**best_estimator_**: estimator <br>\n",
    "Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False. <br>\n",
    "\n",
    "**best_score_** : float <br>\n",
    "Score of best_estimator_ on the left out data.<br>\n",
    "\n",
    "**best_params_**: dict <br>\n",
    "Parameter setting that gave the best results on the hold out data. <br>\n",
    "\n",
    "**best_index_**: int <br>\n",
    "The index (of the `cv_results_` arrays) which corresponds to the best candidate parameter setting. <br>\n",
    "\n",
    "The dict at `search.cv_results_['params'][search.best_index_]` gives the parameter setting for the best model, that gives the highest mean score (`search.best_score_`). <br>\n",
    "\n",
    "**scorer_**: function <br>\n",
    "Scorer function used on the held out data to choose the best parameters for the model. <br>\n",
    "\n",
    "**n_splits_**: int <br>\n",
    "The number of cross-validation splits (folds/iterations).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class methods used in this tutorial: \n",
    "1. `fit(self,X,y,groups,callback)` : Run fit on the estimator with randomly drawn parameters. Only X and y are required, can provide group labels used while splitting the dataset into train/test. \n",
    "2. `predict(self,X)` : Call predict on the estimator with the best found parameters --> predicts labels/values for the provided instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, error_score=-9999,\n",
       "              estimator=Pipeline(memory=None,\n",
       "                                 steps=[('imputer',\n",
       "                                         SimpleImputer(add_indicator=False,\n",
       "                                                       copy=True,\n",
       "                                                       fill_value=None,\n",
       "                                                       missing_values=nan,\n",
       "                                                       strategy='mean',\n",
       "                                                       verbose=0)),\n",
       "                                        ('estimator',\n",
       "                                         RandomForestRegressor(bootstrap=True,\n",
       "                                                               ccp_alpha=0.0,\n",
       "                                                               criterion='mse',\n",
       "                                                               max_depth=None,\n",
       "                                                               max_features='auto',\n",
       "                                                               max_leaf_nodes=None,\n",
       "                                                               max_samples=None,\n",
       "                                                               min_...\n",
       "              search_spaces={'estimator__max_features': Integer(low=1, high=8, prior='uniform', transform='identity'),\n",
       "                             'estimator__min_samples_split': Integer(low=2, high=200, prior='uniform', transform='identity'),\n",
       "                             'estimator__n_estimators': Integer(low=50, high=500, prior='uniform', transform='identity'),\n",
       "                             'imputer__strategy': Categorical(categories=('mean', 'median', 'most_frequent'), prior=None)},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit (train) the model \n",
    "# need to train the model before any of the following steps can be performed\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('estimator__max_features', 6),\n",
       "             ('estimator__min_samples_split', 42),\n",
       "             ('estimator__n_estimators', 375),\n",
       "             ('imputer__strategy', 'most_frequent')])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view best model parameters \n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1251275916733097"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view score of best_estimator on the left out data (i.e. how did the model perform?)\n",
    "# this is the score using the performance metric (scorer) that we provided to BayesSearchCV( ), here is r2\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE TO SELF** need to figure out what the \"left out\" data is, is this the best score of the cross validation splits, when the model is applied to the 10% that was left out (but that the model has already seen)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_estimator__max_features</th>\n",
       "      <th>param_estimator__min_samples_split</th>\n",
       "      <th>param_estimator__n_estimators</th>\n",
       "      <th>param_imputer__strategy</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.054400</td>\n",
       "      <td>0.102235</td>\n",
       "      <td>0.101677</td>\n",
       "      <td>0.034888</td>\n",
       "      <td>0.054914</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>0.135804</td>\n",
       "      <td>0.085493</td>\n",
       "      <td>0.092222</td>\n",
       "      <td>0.111375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.585020</td>\n",
       "      <td>0.535479</td>\n",
       "      <td>0.080290</td>\n",
       "      <td>0.030482</td>\n",
       "      <td>5</td>\n",
       "      <td>142</td>\n",
       "      <td>287</td>\n",
       "      <td>median</td>\n",
       "      <td>{'estimator__max_features': 5, 'estimator__min...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.075334</td>\n",
       "      <td>0.181726</td>\n",
       "      <td>0.202825</td>\n",
       "      <td>0.058129</td>\n",
       "      <td>0.092091</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.134274</td>\n",
       "      <td>0.144350</td>\n",
       "      <td>0.158326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712002</td>\n",
       "      <td>0.197974</td>\n",
       "      <td>0.031392</td>\n",
       "      <td>0.013054</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>137</td>\n",
       "      <td>mean</td>\n",
       "      <td>{'estimator__max_features': 7, 'estimator__min...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059033</td>\n",
       "      <td>0.124095</td>\n",
       "      <td>0.165521</td>\n",
       "      <td>0.064966</td>\n",
       "      <td>0.095709</td>\n",
       "      <td>-0.004723</td>\n",
       "      <td>0.130411</td>\n",
       "      <td>0.130675</td>\n",
       "      <td>0.116789</td>\n",
       "      <td>0.122949</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151458</td>\n",
       "      <td>0.335561</td>\n",
       "      <td>0.049212</td>\n",
       "      <td>0.017882</td>\n",
       "      <td>7</td>\n",
       "      <td>117</td>\n",
       "      <td>231</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>{'estimator__max_features': 7, 'estimator__min...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.060937</td>\n",
       "      <td>0.233634</td>\n",
       "      <td>0.284892</td>\n",
       "      <td>0.048825</td>\n",
       "      <td>0.094346</td>\n",
       "      <td>-0.006292</td>\n",
       "      <td>0.241879</td>\n",
       "      <td>0.111467</td>\n",
       "      <td>0.132016</td>\n",
       "      <td>0.176489</td>\n",
       "      <td>...</td>\n",
       "      <td>2.058423</td>\n",
       "      <td>0.607050</td>\n",
       "      <td>0.085776</td>\n",
       "      <td>0.030330</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>375</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>{'estimator__max_features': 6, 'estimator__min...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.057614</td>\n",
       "      <td>0.110046</td>\n",
       "      <td>0.130069</td>\n",
       "      <td>0.048667</td>\n",
       "      <td>0.067302</td>\n",
       "      <td>0.022258</td>\n",
       "      <td>0.151105</td>\n",
       "      <td>0.092738</td>\n",
       "      <td>0.092853</td>\n",
       "      <td>0.114085</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078048</td>\n",
       "      <td>0.351547</td>\n",
       "      <td>0.057439</td>\n",
       "      <td>0.020748</td>\n",
       "      <td>5</td>\n",
       "      <td>132</td>\n",
       "      <td>217</td>\n",
       "      <td>median</td>\n",
       "      <td>{'estimator__max_features': 5, 'estimator__min...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0          -0.054400           0.102235           0.101677           0.034888   \n",
       "1          -0.075334           0.181726           0.202825           0.058129   \n",
       "2          -0.059033           0.124095           0.165521           0.064966   \n",
       "3          -0.060937           0.233634           0.284892           0.048825   \n",
       "4          -0.057614           0.110046           0.130069           0.048667   \n",
       "\n",
       "   split4_test_score  split5_test_score  split6_test_score  split7_test_score  \\\n",
       "0           0.054914           0.031297           0.135804           0.085493   \n",
       "1           0.092091           0.000007           0.195300           0.134274   \n",
       "2           0.095709          -0.004723           0.130411           0.130675   \n",
       "3           0.094346          -0.006292           0.241879           0.111467   \n",
       "4           0.067302           0.022258           0.151105           0.092738   \n",
       "\n",
       "   split8_test_score  split9_test_score  ...  mean_fit_time  std_fit_time  \\\n",
       "0           0.092222           0.111375  ...       1.585020      0.535479   \n",
       "1           0.144350           0.158326  ...       0.712002      0.197974   \n",
       "2           0.116789           0.122949  ...       1.151458      0.335561   \n",
       "3           0.132016           0.176489  ...       2.058423      0.607050   \n",
       "4           0.092853           0.114085  ...       1.078048      0.351547   \n",
       "\n",
       "   mean_score_time  std_score_time  param_estimator__max_features  \\\n",
       "0         0.080290        0.030482                              5   \n",
       "1         0.031392        0.013054                              7   \n",
       "2         0.049212        0.017882                              7   \n",
       "3         0.085776        0.030330                              6   \n",
       "4         0.057439        0.020748                              5   \n",
       "\n",
       "   param_estimator__min_samples_split  param_estimator__n_estimators  \\\n",
       "0                                 142                            287   \n",
       "1                                  93                            137   \n",
       "2                                 117                            231   \n",
       "3                                  42                            375   \n",
       "4                                 132                            217   \n",
       "\n",
       "   param_imputer__strategy                                             params  \\\n",
       "0                   median  {'estimator__max_features': 5, 'estimator__min...   \n",
       "1                     mean  {'estimator__max_features': 7, 'estimator__min...   \n",
       "2            most_frequent  {'estimator__max_features': 7, 'estimator__min...   \n",
       "3            most_frequent  {'estimator__max_features': 6, 'estimator__min...   \n",
       "4                   median  {'estimator__max_features': 5, 'estimator__min...   \n",
       "\n",
       "   rank_train_score  \n",
       "0                 8  \n",
       "1                 5  \n",
       "2                 6  \n",
       "3                 1  \n",
       "4                 7  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View scores for training and testing of cross-validation splits \n",
    "searchResults = pd.DataFrame(search.cv_results_)\n",
    "searchResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.031092927334830156"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now apply the model to the test data\n",
    "\n",
    "# define test set data\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "# apply the model\n",
    "y_pred = search.predict(X_test)\n",
    "\n",
    "# evaluate performance on test data\n",
    "model_test_score = r2(search, X_test, y_test) \n",
    "model_test_score # why is this negative? Does this mean the model is really bad? Why is it so much worse on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23099783203501978, 0.02355094844797427)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the correlation between ground truth and predicted values with pearson correlation coefficient (PCC)\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(y_test, y_pred) # returns (r, two-tailed p-value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE TO SELF**: why is this such a better score than r2? Shouldn't r2 be the same exact thing??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learn About Feature Importance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn about: <br>\n",
    "A. Gini impurity, and its regression counterpart <br>\n",
    "B. Mean Decrease Impurity, how it's calculated using A, and how to get it for our model. <br>\n",
    "C. Mean Decrease Accuracy, also known as Permutation Importance <br>\n",
    "D. Pros and cons of each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Gini Impurity and Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Impurity\n",
    "Gini impurity is a measure of how good a given split is in a classification tree. First, let's review how classification trees work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the whole dataset. Each *instance* has some data associated with it; these data fall into groups called *features*. Each instance also has a *label* associated with it; this label represents the group to whcih the instance belongs. A classification tree aims to split the data in such a way that at the end, we have a separate group for each type of label. This splitting is done by using the information we have about each instance: the features. Consider the following example:\n",
    "\n",
    "![A decision tree that gives the reccomendation to go or not go to the store.](img/Classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original dataset is hours of the day, during one week. The features associated with each hour are, Out Of Food?, Raining?, and Hungry?. By splitting the data based on what value they have for a certain feature, we can decide if an hour belongs to the category where we should go to the store, or not go to the store. The same logic applies if we're trying to classify things into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does Gini impurity have to do with all this? Well, imagine that we're training a model to classify things using a decision tree. We could use different features at different nodes (these are called splits), and it would give us vastly different outcomes. Some versions of the tree are clearly better than others, because depending on which features we put where, we might mis-classify many instances. But what if we have lots of features, and it isn't intuitive which set of splits is better than another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following figure, let's walk through a simple explanation of Gini impurity, adapted from [this post](https://victorzhou.com/blog/gini-impurity/).\n",
    "\n",
    "![A classification decision tree. 7 of 10 datapoints have arrived at node j, 5 red and 2 purple. The daughter nodes perfectly split the data, so one has 5 red dots and the other has 2 purple. 3 red dots are at the suster node to node j](img/Gini.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini impurity is calculated for each split (also called a node). Imagine we want to calculate the Gini impurity of node $n_{j}$. The dots around each node represent the data points (instances) that have arrived to that node by following the previous splits in the tree. Here, purple and red represent the two different classes we're trying to classify. Our simple explanation is the following: for each split,\n",
    "\n",
    "1. Randomly pick a point within the split \n",
    "2. Randomly classify it according to the class distribution within that split (e.g. if there are 10 points, 5 from Class A and 5 from Class B, half the time we’d classify it as A, the other half B)\n",
    "3. Determine the probability that we classify the datapoint incorrectly. This is the Gini Impurity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is summarized in the following table for our example. First, we determine the probability each even occuring: choosing a point of that color (Step 1), and then classifying it correctly or incorrectly (Step 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Event | Classification| Math| Probability   |\n",
    "|:------:|:------:|:------:|:-----:|\n",
    "| Pick red, classify red | Correct|  $\\frac{5}{7}$ \\* $\\frac{5}{7}$| 0.51|\n",
    "| Pick red, classify purple | Incorrect| $\\frac{5}{7}$ \\* $\\frac{2}{7}$| 0.20 |\n",
    "| Pick purple, classify purple | Correct| $\\frac{2}{7}$ \\* $\\frac{2}{7}$| 0.08 |\n",
    "| Pick purple, classify red | Incorrect | $\\frac{2}{7}$ \\* $\\frac{5}{7}$| 0.20 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we take the event probabilities for the incorrect classifications and add them together to get the Gini impurity (Step 3). \n",
    "\n",
    "$$\\textrm{Gini impurity for } n_{j} = 0.20+0.20=0.40$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was possible for a simple example, but what about more complicated ones? Now let's talk about the mathematical definition of Gini impurity. <br>\n",
    "\n",
    "**Definition**: $$\\sum\\limits_{i=1}^C p(i)(1-p(i))$$\n",
    "\n",
    "where $p(i)$ is the probability of picking a datapoint with class $i$, and $C$ is the total number of classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use our example above with this formula: \n",
    "\n",
    "$$p(red)(1-p(red)) + p(purple)(1-p(purple))$$\n",
    "$$\\frac{5}{7}(1-\\frac{5}{7})) + \\frac{2}{7}(1-\\frac{2}{7}))$$\n",
    "$$0.20+0.20$$\n",
    "$$0.40$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get the same result with this formula!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, what is Gini impurity? It's the probability of classifying something incorrectly at a given node. So the lower the impurity, the better we are at making predictions at that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impurity for regressions\n",
    "As you can see, this method only works if we are working with a classification problem. What about regressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review how regression trees work. This section is adapted from [this absolutely wonderful lecture](http://www.stat.cmu.edu/~cshalizi/350-2006/lecture-10.pdf). <br>\n",
    "\n",
    "In a linear regression, we fit one model (predictive formula) over the entire data-space. However, if the data has lots of features which interact in complicated, nonlinear ways, it’s very difficult to fit a good linear model, and because of all the terms you would have to add to the equation in order to get a fit over the whole data space, the resulting model is extraordinarily complicated and uninterpretable. <br>\n",
    "\n",
    "Therefore, we can do something called recursive partitioning. Instead of fitting a global model, we can subdivide, or partition, the dataspace into smaller regions. We continue splitting these partitions (which is why it’s called recursive partitioning) until we have manageable chunks of data-space where we can fit simple models. The division of the dataspace is done by features, the same as for a classification tree. <br>\n",
    "\n",
    "Regression trees are a way of representing recursive partitioning. Each of the terminal leaves of the tree represents a final partition, which has attached to it a simple model, as illustrated below. <br>\n",
    "\n",
    "![A regression decision tree for predicting car prices based on horesepower and wheelbase. Each edge contains an inequality indicating the value that feature was split on; for example, if the horsepower was split at 1.3, the two edges would have the inequalities \"less than 1.3\" and \"greater than 1.3\". The leaves (terminal nodes) have the predicted price, and in parenthesis the number of instances arrived at that leaf. The internal nodes have the name of the feature on which the node was split. The caption reads:  Regression tree for predicting price of 1993-model cars. All features\n",
    "have been standardized to have zero mean and unit variance. Note that the order\n",
    "in which variables are examined depends on the answers to previous questions.\n",
    "The numbers in parentheses at the leaves indicate how many cases (data points)\n",
    "belong to each leaf.](img/regression.png)\n",
    "\n",
    "In the above, the word at each node is the feature on which the data is being split. These are continuous features, and the partition is made by choosing a value of that feature at which to split the data. As you can see, Horsepower shows up three times, and each time, a different value is used to make the split.You can get a more intuitive sense of this by looking at the following figure: <br>\n",
    "\n",
    "![A 2 dimensional scatter plot with vertical and horizontal lines partitioning the data. Within each partition is the mean for the quantity of interest of the points within the partition. The caption for the figure reads: The partition of the data implied by the regression tree from Figure 1.\n",
    "Notice that all the dividing lines are parallel to the axes, because each internal\n",
    "node checks whether a single variable is above or below a given value.](img/Partitions.png)\n",
    "\n",
    "When there are more than two features, we are looking for partitions ina higher dimensional space. <br>\n",
    "\n",
    "What are the simple local models? In a classic regression tree, this is just the sample mean of the dependent variable (from the training data) within the given partition. In the decision tree this is represented as \"Price = (some number)\", and in the partition figure, it is the number in red within each partition. This means a classic regression tree is a piecewise-constant model. Since this is easy to calculate once a tree is decided upon, all effort should go into finding the best set of partitions. <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with how splits are made in a regression tree, let's talk about the regression version of Gini impurity. It's actually much simpler: it's just Mean Square Error (MSE). This explanation is adapted from [this post](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3) on how feature importances are implemented in scikit-learn. <br>\n",
    "\n",
    "**Definition**:\n",
    "\n",
    "$$\\frac{1}{N} \\sum\\limits_{i=1}^N (y_{i} - \\mu)^2$$\n",
    "\n",
    "Where $y_{i}$ is the label for an instance (its value), $N$ is the number of instances and $\\mu$ is the mean given by $\\frac{1}{N} \\sum\\limits_{i=1}^N y_{i}$. In plain english, MSE is the mean of the squared differences (error) between the true value (instance label) and the group mean at the node ($\\mu$), for all instances at the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a simple example, using the following tree: \n",
    "\n",
    "![A decision tree where 7 of 10 instances have arrived at node j, the numbers are 1, 3, 4, 5, 8, 9, and the daughter nodes contain 1, 3, 4, and 5, 8, 9, respectively.](img/MSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the MSE of $n_{j}$, we first have to calculate $\\mu$ at $n_{j}$. \n",
    "\n",
    "$$\\mu = \\frac{1+3+4+5+8+9}{6} = 5$$\n",
    "\n",
    "Then, we follow the formula:\n",
    "\n",
    "$$\\frac{1}{6}*((1-5)^2 + (3-5)^2 + (4-5)^2 + (5-5)^2 + (8-5)^2 + (9-5)^2) = 7.67$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does MSE mean intuitively, after we've worked through the above example? We could say that, at the end of a regression tree, predictions are made using the mean of all instances at the node. Therefore, the difference between a given instance label and the mean at the node is the error, because that’s how far off the prediction for that instance could be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Mean Decrease Impurity\n",
    "We can calculate impurity at nodes now, but so what? How can we use this to evalulate how good our tree is (and later on, our Random Forest model) at classification or regression? We can use Gini impurity and MSE to define something called Mean Decrease Impurity (MDI), which tells us how good a given feature is at helping to classify the data. MDI follows the same procedure for both classification and regression, and just uses different impurity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm\n",
    "\n",
    "A short description of how this is implemented in scikit-learn (from the same source above):<br>\n",
    "“Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.”  <br>\n",
    "\n",
    "What on earth does that mean? Let's break it down into steps. <br>\n",
    "\n",
    "**1. Calculate the importance of each node.**\n",
    "\n",
    "In order to do this, we use the following equation: \n",
    "\n",
    "$$ni_{j} = w_{j} C_{j} -  w_{left(j)} C_{left(j)} -  w_{right(j)} C_{right(j)}$$\n",
    "\n",
    "where <br>\n",
    "$ni_{j}$ = the importance of node $j$ <br>\n",
    "$w_{j}$ = weighted number of samples reaching node $j$ <br>\n",
    "$C_{j}$ = the impurity value of node $j$ (either Gini impurity or MSE) <br>\n",
    "$left(j)$ = child node from the left of the split made at node $j$ <br>\n",
    "$right(j)$ = child node from the right of the split made at node $j$ <br>\n",
    "\n",
    "In plain english: Node importance = (number of samples that reach this node ÷ total number of samples)\\*(Gini impurity of this node) - (number of samples that reach the left child of this node ÷ total number of samples)\\*(Gini impurity of the left child of this node) - (number of samples that reach the right child of this node ÷ total number of samples)\\*(Gini impurity of the right child of this node) <br>\n",
    "\n",
    "**2. Calculate the feature importance within this individual tree.**\n",
    "\n",
    "In order to do this, we use the following:\n",
    "\n",
    "$$fi_{i} = \\frac{\\sum\\limits_{j:\\textrm{node }j \\textrm{ splits on feature }i} ni_{j}}{\\sum\\limits_{k \\in all nodes} ni_{k}}$$\n",
    "\n",
    "where <br>\n",
    "$fi_{i}$ = the importance of feature $i$ <br>\n",
    "\n",
    "In plain english: Feature importance = (the sum of node importances for nodes that split on this feature) ÷ (sum of all node importances) \n",
    "\n",
    "**3. Normalize feature importance.**\n",
    "\n",
    "We normalize feature importances to a value between 0 and 1 by dividing by the sum of all feature importances.\n",
    "\n",
    "**4. Average feature importance across all trees.**\n",
    "\n",
    "Since a Random Forest is made up of many individual decision trees, we get overall feature importance by taking the sum of the importances for that feature, divided by the number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros and cons of MDI\n",
    "\n",
    "There are many ways to calculate feature importance, more of which you will learn further on in this tutorial. Why should we use MDI? The main reason MDI is so widespread in evaluating Random Forest models, is because it's easy to calculate on-the-go as the model is trained, and doesn't require a lot of extra computational power. However, MDI has a major downfall: it is biased heavily towards variables with multiple splits. <br>\n",
    "\n",
    "Recall how we make our decision trees: we split the data using features. If the feature is binary, for example, Am I Hungry? There's only one way to split the data using that feature. However, if the variable is continuous, like Horsepower, it can be split on any value in its distribution, and will be re-used many times to try out many different splits, even if in reality its not that good at splitting the data. Because MDI is higher for features that are used more often, this artificially biases the feature importance of continuous features, or discrete features with more than two categories, higher than binary features. This is why we have to be wary of MDI as an importance measure, especially if our features are of mixed types. Further explanation can be found [here](https://blog.methodsconsultants.com/posts/be-aware-of-bias-in-rf-variable-importance-metrics/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the theory of Mean Decrease Impurity, how do we apply this to our model? <br>\n",
    "\n",
    "scikit-learn automatically calculates feature importances using MDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29335981, 0.31970761, 0.02847384, 0.05192288, 0.07050175,\n",
       "       0.07827273, 0.08667251, 0.07108886])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reminder: search is a BayesSearchCV object, the function used the Pipeline object to make the \"search\" object\n",
    "# self.best_estimator_ is: Estimator that was chosen by the search, \n",
    "# i.e. estimator which gave highest score (or smallest loss if specified) on the left out data.\n",
    "\n",
    "# named steps is a dict, keys are names and values are objects, \n",
    "# they are the steps we included in our Pipeline object\n",
    "# The pipline has 1 imputer and  1 estimator step\n",
    "\n",
    "imputer   = search.best_estimator_.named_steps['imputer']  \n",
    "estimator = search.best_estimator_.named_steps['estimator'] \n",
    "\n",
    "estimator.feature_importances_ # these are the mean importances across the whole Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'Importance'),\n",
       " Text(0.5, 1.0, 'Feature Importance Distributions')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tree.feature_importances_ is from scikit learn, so these are MDI importances \n",
    "\n",
    "# get the feature importances from each individual tree and then visualize the distributions as boxplots\n",
    "all_feat_imp_df = pd.DataFrame(data=[tree.feature_importances_ for tree in \n",
    "                                     estimator],\n",
    "                               columns=features)\n",
    "\n",
    "sns.boxplot(data=all_feat_imp_df).set(title='Feature Importance Distributions',\n",
    "             ylabel='Importance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Forty and Weight are the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Mean Decrease Accuracy/Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn docs](https://scikit-learn.org/stable/modules/permutation_importance.html): <br>\n",
    "\n",
    "\"The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use [this tutorial from kaggle](https://www.kaggle.com/dansbecker/permutation-importance) and the scikit-learn docs to break down the above definition. <br>\n",
    "\n",
    "In permutation importance, all we do is scramble one feature, so that the relationship between the instance label and the feature value is random, as seen in the image below. <br>\n",
    "\n",
    "![A table of features, where one column is being scrambled](img/permutation.png)\n",
    "\n",
    "If a feature is important, when we scramble it and lose the relationship that gives the feature its predictive power, we will see a greater *decrease in accuracy*, thus the name Mean Decrease Accuracy (MDA). The greater the loss in performance upon permuting a given feature, the more important that feature is. Since there is randomness in the exact performance change of a model upon shuffling, we repeat the process multiple times on each feature with different shuffles, allowing us to measure the stochastity in our calculation. Strictly speaking, we would expect the MDA for a given feature not to be less than 0, because you wouldn't expect model performance to increase upon permuting a feature. However, negative values are occasionally obtained, and just means that the feature is not very important, and the random shuffling of the data actually *improved* the model performance. <br>\n",
    "\n",
    "One of the great benefits of MDA is that it is *model agnostic*, meaning that it can be applied to models other than decision trees, including difficult-to-interpret black box models. Additionally, it is not biased towards continuous (numerical) features. However, it still gives high importance to features that allow overfitting, even if those features do not contribute to good predictions on unseen data. \n",
    "\n",
    "Additionally, we have to take into account strongly correlated features. Since we permute one feature at a time, if we permute a feature that is highly correlated to another, the model will still be able to make predictions without a large decrease in performance. This is because through the correlation, the model still has access to the feature we've permuted, and results in a lower score for both features, even if they may be important. \n",
    "\n",
    "**One more important thing emphasized by the sciki-learn docs:** <br>\n",
    "\n",
    "\"Note that features that are deemed non-important for some model with a low predictive performance could be highly predictive for a model that generalizes better. The conclusions should always be drawn in the context of the specific model under inspection and cannot be automatically generalized to the intrinsic predictive value of the features by themselves. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances.\" <br>\n",
    "\n",
    "AKA, make sure your model isn't overfit before calculating feature importances!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the package `eli5` to calculate permutation importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wt</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>0.032250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forty</td>\n",
       "      <td>0.237577</td>\n",
       "      <td>0.031657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cone</td>\n",
       "      <td>0.040714</td>\n",
       "      <td>0.005697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shuttle</td>\n",
       "      <td>0.036012</td>\n",
       "      <td>0.005826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BroadJump</td>\n",
       "      <td>0.032836</td>\n",
       "      <td>0.005403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BenchReps</td>\n",
       "      <td>0.029503</td>\n",
       "      <td>0.004742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vertical</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ht</td>\n",
       "      <td>0.009884</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature    weight       std\n",
       "0         Wt  0.303350  0.032250\n",
       "1      Forty  0.237577  0.031657\n",
       "2       Cone  0.040714  0.005697\n",
       "3    Shuttle  0.036012  0.005826\n",
       "4  BroadJump  0.032836  0.005403\n",
       "5  BenchReps  0.029503  0.004742\n",
       "6   Vertical  0.022869  0.003648\n",
       "7         Ht  0.009884  0.001659"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "# we need to impute the data first before calculating permutation importance\n",
    "train_X_imp = imputer.transform(X)\n",
    "\n",
    "# set up the met-estimator to calculate permutation importance on our training data\n",
    "perm_train = PermutationImportance(estimator, scoring=r2,\n",
    "                                   n_iter=50, random_state=RANDOM_STATE)\n",
    "# fit and see the permuation importances\n",
    "perm_train.fit(train_X_imp, y)\n",
    "eli5.explain_weights_df(perm_train, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distributions\n",
    "perm_train_feat_imp_df = pd.DataFrame(data=perm_train.results_,\n",
    "                                      columns=features)\n",
    "(sns.boxplot(data=perm_train_feat_imp_df)\n",
    "        .set(title='Permutation Importance Distributions (training data)',\n",
    "             ylabel='Importance'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Pros and Cons\n",
    "\n",
    "We've already discussed some of the pros and cons of each method, but now let's take a look at the two methods side by side. \n",
    "\n",
    "| Attribute | Mean Decrease Impurity| Mean Decrease Accuracy (Permutation Importance) |\n",
    "|:------:|:------:|:------:|\n",
    "|Model agnostic?|N|Y|\n",
    "|Biased towards continous features?|Y|N|\n",
    "|Speed/convenience?|Calculated on-the-go|Need to retrain model for each permutation|\n",
    "|Susceptible to correlated features?|[?](https://www.sciencedirect.com/science/article/pii/S0167947307003076?casa_token=3vWzXmNdC0MAAAAA:6Ugu68jnWcU_yZgRMXCyceRNHMDQA3HE2PHKflheLrciLIBZykiYEZNjAwa0GdczMGket24vCfg)|Y|\n",
    "\n",
    "As you can see, the main advantage of Gini importance is that it's quick to calculate. However, if you need accurate feature importances for a set of features that are of mixed type, permutation importance is the way to go, even though it takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO double check the information in this table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we've talked about feature importances, whose values indicate how important a given feature is for the model's performance. However, it's not necessarily intuitive basedon feature importances alone, how our model made its decisions or predictions. In order to have a better idea about how specific predictions were made, we'll use something called *feature contributions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Feature Contributions\n",
    "\n",
    "In this section you will learn about: <br>\n",
    "A. The mathematics of calculating feature contributions (The mathematics of feature contributions)<br>\n",
    "B. The intuitive explanation of feature contributions (So what does this actually mean?)<br>\n",
    "C. How to analyze feature contributions (Analyzing feature contributions)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. The mathematics of feature contributions\n",
    "\n",
    "This section is adapted from [this paper](https://arxiv.org/pdf/1312.1121.pdf) on feature contributions, and will take a decently deep dive into the math behind the Random Forest tree growing and decision process as well as that of calculating feature contributions. If you want to skip the math but get the general idea of what's going on, skip to the section: **B. So what does this actually mean?** <br> \n",
    "\n",
    "#### Understanding classification models\n",
    "\n",
    "In order to understand feature contributions, we first have to understand how Random Forest works. The first part of this section is dedicated to understanding this process for classification models. <br>\n",
    "\n",
    "In section 2, we went over how classification and regression trees (CART) work, and briefly mentioned how they are combined together in an ensemble to make a Random Forest. But what is actually happening? In short, during training/tree building, each decision tree in the forest only has access to a random subset of the data, and only a random subset of the features. In order to make a prediction, the individual trees all make a decision, and the final classification is decided upon by a vote. In essence, a bunch of crappy trees making a collective decision has better performance than one really good tree. If you are interested in a probabalistic mathematical explanation of how this works, please see the section **Probabalistic Interpretation of Classification Random Forest Models** in the supplemental.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk briefly about the proecss of *growing* a decision tree. In a Random Forest, each tree has access to only a random subset of the training data and a random subset of features. Once that training set is selected, a feature is used to split the data. The instances that arrive at a particular node after being split at a parent node are called *training instances*. This process is repeated iteratively until Gini impurity at the latest node is 0 (all instances at the node are of the same class), or further splitting doesn't improve the classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE TO SELF:** *if there are only two classes, won't one of them always have a probability of more than 0.5 at any node in the tree? If so, how is any terminal node used to classify C2? \"We will refer to instances of the local training set that pass through a given node as the training instances in this node. The fraction of the training instances in a node n belonging to class C1 will be denoted by Ynmean. This is the probability that a randomly selected element from the training instances in this node is in the first class. In particular, a terminal node is assigned to class C1 if Ynmean > 0.5 or Ynmean = 0.5 and the draw is resolved in favor of class C1.\"* <br> \n",
    "\n",
    "*Is this just what happens at terminal nodes where further splitting won't improve prediction? How does the algorithm know that further splitting won't improve the prediction? * <-- taking this as the ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO make sure the above is correct and if not, change the explanation below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Gini impurity of a node is 0, then we classify all instances arriving at that node according to whatever class all of those training instances at the terminal node belong to. But what happens if Gini impurity isn't 0, and further splitting doesn't improve the classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a given node $n$, the fraction of training instances belonging to class $C_{1}$ is denoted by $Y^{n}_{mean}$. This represents the probability that a randomly selected training instance at that node is of class $C_{1}$. If we've come to a terminal node that is impure, where further splitting won't improve our classification, this value is used to determine how that terminal node will classify any instances that arrive there by following the tree. If $Y^{n}_{mean} > 0.5$, that node will classify instanceas as $C_{1}$, or if $Y^{n}_{mean} == 0.5$ (a draw, since there are only two classes) and the draw is resolved (randomly) in favor of $C_{1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we know how trees are built and how instances are classified, let's talk about feature contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Contributions: Classification\n",
    "\n",
    "In order to determine the feature contribution for a given instance predicted by a Random Forest, we have to follow two steps. First, we have to look at local feature contributions for each individual tree, and then second, aggregate them over the Forest. <br> \n",
    "\n",
    "Local contributions are determined using the following. First, we calculate the local increment for a \n",
    "\n",
    "$$ LI^{c}_{f} =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      Y^{c}_{mean} - Y^{p}_{mean} & \\textrm{if the split in the parent\n",
    "is performed over the\n",
    "feature f} \\\\\n",
    "      0 & \\textrm{otherwise} \\\\\n",
    "\\end{array} \n",
    "\\right. $$\n",
    "\n",
    "where $LI^{c}_{f}$ is the local increment of feature $f$ at node $c$. A local increment can be intuitively thought of as the contribution a feature makes at a node towards correctly classifying an instance. In fact, if we look at the $Y_{mean}$ of the root of the tree, and the $Y_{mean}$ of a terminal node for an instance that ends up at that terminal node, we can see that the sum of the local increments for all nodes along that path is equal to the difference between the two $Y_{mean}$s -- which shows that those increments are directly responsible for the classification at the end of the tree. <br> \n",
    "\n",
    "For a given feature $f$, we can now calculate that feature's contribution to the prediction of a given instance $i$, by summing the $LI_{f}$ over the whole decision path for $i$:\n",
    "\n",
    "$$FC^{f}_{i,t} = \\sum\\limits_{s = 0}^S LI_{f}$$\n",
    "\n",
    "where $FC^{f}_{i,t}$ is the contribution of feature $f$ to predicting istance $i$ in a given tree $t$, and $S$ is the number of splits in the decision path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our local contributions in individual trees, we have to aggregate them over the Forest. This is done using the following formula: \n",
    "\n",
    "$$FC^{f}_{i} = \\frac{1}{T} \\sum\\limits_{t=1}^T FC^{f}_{i,t}$$\n",
    "\n",
    "where $FC^{f}_{i}$ is the feature contribution of feature $f$ over the whole Forest to predicting instance $i$, and $T$ is the number of trees in the forest. We can then make a feature contributions vector for instance $i$, which is just a vector of $FC^{f}_{i}$ for all features used in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to mention here is something called the *unanimity condition*. This is when all terminal nodes in a tree or Forest are pure -- all training instances that have arrived to a given terminal node are of the same class. When this condition holds, we can use the feature contributions to recover the predictions in the model. Since this is only relevant to classification models, and in this tutorial we're dealing with a regression problem, further explanation can be found in the Supplemental, in the section **The Unanimity Condition: Using feature contributions to retrieve model predictions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Contributions: Regressions\n",
    "\n",
    "Now that's great, you're thinking, but what about regression trees? The method for computing feature contributions for a regression tree or Random Forest regression is analagous to that of a classification, with some slight differences. This section is adapted from the description found in [this blog post](http://blog.datadive.net/interpreting-random-forests/), which the module `treeinterpreter` is based on.<br>\n",
    "\n",
    "The main difference is that for a regression, $Y^{n}_{mean}$, rather than being the fraction of training instances belonging to a given class, is just the mean of the training instances at a given node. For example, if a node has instances with the labels $(3,4,6,7)$, then the $Y^{n}_{mean}$ for that node is $5$. <br>\n",
    "\n",
    "Therefore, the process is exactly the same as for classifications -- except that $LI^{c}_{f}$ represents the difference in mean of the training samples at two nodes, rather than the difference in probability of correct classification at a node. <br>\n",
    "\n",
    "This also means we can use feature contributions to directly recover the predictions made by the model, since the feature contributions along a given decision path represent the change in mean (what we can think of as our prediction, since the mean at the terminal node is the prediction) from the previous to the current node. The equation that represents this is as follows: \n",
    "\n",
    "$$f(x) = c_{full} + \\sum\\limits^{K}_{k=1} contrib(x,k)$$\n",
    "\n",
    "where $f(x)$ is our prediction, $c_{full}$ is the mean of all training instances at the root node, and $contrib(x,k)$ is the contribution of feature $k$ to the prediction of instance $x$. If we were to do this for a whole forest, we would simply be using the mean of the means at all root nodes, and the average contributions of features across the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. So what does this actually mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this all together. We can represent the process of calculating feature contributions with the following schematic: \n",
    "\n",
    "![visual representation of feature contribution calculation](img/featureContributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an instance $i$, we follow its decision path through the tree and calculate local increments at each node on the path. If the feature of interest, $f$, splits the node, the node receives a local increment that is greater than 1, the calculation of which can be found in the previous section. If the node is split over some other feature, the local increment is 0. We then sum up these local increments for the entire decision path, which gives us the feature contribution for this tree. We do this for every tree in the Forest, and then average the feature contributions, in order to get the overall feature contribution to the prediction made by the Forest.\n",
    "\n",
    "This general schematic holds for both regressions and classification problems, the only difference is the way of calculating the local increment at each node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Analyzing feature contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_interpretability]",
   "language": "python",
   "name": "conda-env-ML_interpretability-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
